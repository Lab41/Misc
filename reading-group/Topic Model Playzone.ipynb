{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import pyspark.mllib.clustering\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This block of code is only necessary if you are running locally\n",
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.sql import SQLContext \n",
    "\n",
    "class SCSingleton(object):\n",
    "    \"\"\" Wrapper for Spark Context to prevent multiple instantiation of the Spark Context. \"\"\"\n",
    "    \n",
    "    __instance = None\n",
    "\n",
    "    def __new__(cls):\n",
    "        if SCSingleton.__instance is None:\n",
    "            SCSingleton.__instance = object.__new__(cls)\n",
    "            #SCSingleton.__instance.conf = conf\n",
    "            SCSingleton.__instance.sc = SparkContext()\n",
    "            SCSingleton.__instance.sqlCtx = SQLContext(SCSingleton.__instance.sc)\n",
    "        else:\n",
    "            print \"ERROR: An instance of a SparkContext is already created. \" + \\\n",
    "                          \" That instance is \", SCSingleton.__instance.conf\n",
    "        return SCSingleton.__instance\n",
    "\n",
    "singleton = SCSingleton()\n",
    "sc = singleton.sc\n",
    "sqlCtx = singleton.sqlCtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lda_corpus(data):\n",
    "        \n",
    "    documents_df = sc.parallelize(data)\n",
    "    \n",
    "    #choose a vocabulary - in this case using gensim to make a dictionary\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    \n",
    "    #filter out very frequent terms - could also use an exclusion list\n",
    "    dictionary.filter_extremes(no_below = 1, no_above=0.90, keep_n=None)   \n",
    "    \n",
    "    num_terms = len(dictionary.keys())\n",
    "    tokens = dictionary.token2id\n",
    "    \n",
    "    print 'Using dictionary:'\n",
    "    sorted_names = sorted(tokens, key=tokens.__getitem__)\n",
    "    print sorted_names\n",
    "    #Sorted_names is handy because the lda_corpus will only return the name index and not the name with the model\n",
    "    #By returning both we have the name and its corresponding index\n",
    "    \n",
    "    lda_corpus = documents_df.map(lambda row: vectorize_by_dict(row, num_terms, tokens))\\\n",
    "        .zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "        \n",
    "    return lda_corpus, sorted_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def look_at_topics(lda_corpus, sorted_names, num_topics, num_words, \\\n",
    "                   maxIterations=20, docConcentration=-1.0, topicConcentration=-1.0, \\\n",
    "                   seed=None, checkpointInterval=10, optimizer='em'):\n",
    "    \n",
    "    # Cluster the documents into topics using LDA\n",
    "    \n",
    "    #defaults: k=10, maxIterations=20, docConcentration=-1.0, topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer='em'\n",
    "    ldaModel = LDA.train(lda_corpus, num_topics, maxIterations, docConcentration, topicConcentration, \\\n",
    "                    seed, checkpointInterval, optimizer)\n",
    "    \n",
    "    #ldaModel = LDA.train(lda_corpus, k=num_topics)\n",
    "    \n",
    "    #print ldaModel.describeTopics()\n",
    "    \n",
    "    #The Topics Matrix is the weight of each word for each topic\n",
    "    #For a given word the total weight overall topics equals the number of times the word is in the corpus\n",
    "    #Renormalize the topics matrix to view the top words by topic\n",
    "    topicMatrix = ldaModel.topicsMatrix()\n",
    "    topicMatrix.flags.writeable = True\n",
    "    word_sums = np.sum(topicMatrix, axis=1)\n",
    "    topics = topicMatrix / word_sums[:,None]\n",
    "    \n",
    "    print \"Top %i words by Topic:\"%num_words\n",
    "    for topic in range(num_topics):\n",
    "        word_index = zip(*sorted(enumerate(np.transpose(ldaModel.topicsMatrix())[topic]), \\\n",
    "                                 key=operator.itemgetter(1)))[0][-num_words:]\n",
    "        print(\"Topic \" + str(topic) + \":\")\n",
    "        word_index = word_index[::-1]\n",
    "        for idx in word_index:\n",
    "            print (\" \" + sorted_names[idx] +\" \"+ str(topics[idx][topic]))\n",
    "        \n",
    "    #print '\\n'\n",
    "    \n",
    "    #Uncomment this block of code if you want to describe all words in each topic with their weights\n",
    "    # Output topics. Each is a distribution over words (matching word count vectors)\n",
    "#     print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize()) + \" words):\")\n",
    "#     topics = ldaModel.topicsMatrix()\n",
    "#     for topic in range(num_topics):\n",
    "#         print(\"Topic \" + str(topic) + \":\")\n",
    "#         for word in range(0, ldaModel.vocabSize()):\n",
    "#             print(\" \" + str(sorted_names[word]) +\" \"+ str(topics[word][topic]))\n",
    "\n",
    "    return ldaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the document's topics by summing the normalized word weights for each word in the document\n",
    "#Then divide by the number of words to get the topic fraction (so weights for a document over all topics will equal 1)\n",
    "def doc_to_topic(docs, ldaModel, sorted_names):\n",
    "    num_topics = ldaModel.topicsMatrix().shape[1]\n",
    "    \n",
    "    \n",
    "    topicMatrix = ldaModel.topicsMatrix()\n",
    "    topicMatrix.flags.writeable = True\n",
    "    word_sums = np.sum(topicMatrix, axis=1)\n",
    "    topics = topicMatrix / word_sums[:,None]\n",
    "    \n",
    "    doc_topics = []\n",
    "    for doc in docs:\n",
    "        doc_topic_weights = []\n",
    "        for topic in range(num_topics):\n",
    "            num_words = 0\n",
    "            total_weight = 0\n",
    "            for word in doc:\n",
    "                try:\n",
    "                    word_idx = sorted_names.index(word)\n",
    "                    word_topic_weight = topics[word_idx][topic]\n",
    "                    num_words += 1\n",
    "                    total_weight += word_topic_weight\n",
    "                except:\n",
    "                    pass\n",
    "            topic_weight = total_weight/num_words\n",
    "            doc_topic_weights.append(topic_weight)\n",
    "        \n",
    "        doc_topics.append(doc_topic_weights)\n",
    "    return doc_topics\n",
    "            \n",
    "def vectorize_by_dict(row, term_count, tokens):\n",
    "    ar = np.zeros(term_count)\n",
    "    for word in row:\n",
    "        try:\n",
    "            idx = tokens[word]\n",
    "            ar[idx] += 1\n",
    "        except:\n",
    "            pass\n",
    "    return Vectors.dense(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toy_data = [('apple', 'orange'),\n",
    "            ('apple', 'banana'),\n",
    "            ('banana', 'orange'),\n",
    "            ('tiger', 'cat'),\n",
    "            ('cat', 'dog'),\n",
    "            ('dog', 'tiger')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dictionary:\n",
      "[u'apple', u'dog', u'cat', u'tiger', u'orange', u'banana']\n"
     ]
    }
   ],
   "source": [
    "lda_corpus, sorted_names = get_lda_corpus(toy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 words by Topic:\n",
      "Topic 0:\n",
      " orange 0.564960882549\n",
      " apple 0.537878401362\n",
      " tiger 0.50677397809\n",
      "Topic 1:\n",
      " dog 0.559068651179\n",
      " cat 0.540407844128\n",
      " banana 0.510141565969\n"
     ]
    }
   ],
   "source": [
    "ldaModel = look_at_topics(lda_corpus, sorted_names, num_topics=2, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.55141964195520521, 0.44858035804479479],\n",
       " [0.51386841769648883, 0.48613158230351128],\n",
       " [0.52740965828997044, 0.47259034171002962],\n",
       " [0.4831830669811345, 0.51681693301886555],\n",
       " [0.4502617523465467, 0.54973824765345336],\n",
       " [0.47385266345568866, 0.52614733654431134]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_topic(toy_data, ldaModel, sorted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([4, 0, 3, 5, 2, 1],\n",
       "  [0.1883205954503354,\n",
       "   0.1792930872795163,\n",
       "   0.1689249296024874,\n",
       "   0.16328640589587137,\n",
       "   0.15319763036984826,\n",
       "   0.14697735140194132]),\n",
       " ([1, 2, 5, 3, 0, 4],\n",
       "  [0.1863559189352009,\n",
       "   0.18013565986914823,\n",
       "   0.1700469166222787,\n",
       "   0.16440841095618422,\n",
       "   0.15404028645219528,\n",
       "   0.14501280716499268])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Describe topics is the word index and their weights for each topic\n",
    "ldaModel.describeTopics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Now that it works for a small example - we can also go through a directory listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_directory = 'test_docs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_splitter(row):\n",
    "    str_row = str(row)\n",
    "    exclude = set(string.punctuation)\n",
    "    str_row_clean = ''.join(ch for ch in str_row if ch not in exclude)\n",
    "    return str_row_clean.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_docs = sc.emptyRDD()\n",
    "for i in os.listdir(corpus_directory):\n",
    "    doc = sc.textFile(corpus_directory+i)\n",
    "    split_doc = doc.map(lambda row: doc_splitter(row))\n",
    "    if all_docs.isEmpty():\n",
    "        all_docs = split_doc\n",
    "    else:\n",
    "        all_docs = all_docs.union(split_doc)\n",
    "docs= all_docs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dictionary:\n",
      "[u'brown', u'lazy', u'slow', u'clouds', u'sloth', u'very', u'jumped', u'over', u'tree', u'dog', u'yellow', u'gently', u'fox', u'overhead', u'pretty', u'quick', u'climed', u'roll']\n",
      "Top 3 words by Topic:\n",
      "Topic 0:\n",
      " over 0.535308496426\n",
      " dog 0.534304742768\n",
      " lazy 0.52703040387\n",
      "Topic 1:\n",
      " slow 0.530950123118\n",
      " sloth 0.527294647625\n",
      " roll 0.52548928296\n"
     ]
    }
   ],
   "source": [
    "lda_corpus_docs, sorted_names_docs = get_lda_corpus(docs)\n",
    "ldaModel = look_at_topics(lda_corpus_docs, sorted_names_docs, num_topics=2, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.52664540662037762, 0.47335459337962238],\n",
       " [0.47793136062292624, 0.52206863937707371]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_to_topic(docs, ldaModel, sorted_names_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
